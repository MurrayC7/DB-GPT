# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-17 21:23+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy/deploy.md:1
#: de0b03c3b94a4e2aad7d380f532b85c0
msgid "Installation From Source"
msgstr "源码安装"

#: ../../getting_started/install/deploy/deploy.md:3
#: 65a034e1a90f40bab24899be901cc97f
msgid ""
"This tutorial gives you a quick walkthrough about use DB-GPT with you "
"environment and data."
msgstr "本教程为您提供了关于如何使用DB-GPT的使用指南。"

#: ../../getting_started/install/deploy/deploy.md:5
#: 33b15956f7ef446a9aa4cac014163884
msgid "Installation"
msgstr "安装"

#: ../../getting_started/install/deploy/deploy.md:7
#: ad64dc334e8e43bebc8873afb27f7b15
msgid "To get started, install DB-GPT with the following steps."
msgstr "请按照以下步骤安装DB-GPT"

#: ../../getting_started/install/deploy/deploy.md:9
#: 33e12a5bef6c45dbb30fbffae556b664
msgid "1. Hardware Requirements"
msgstr "1. 硬件要求"

#: ../../getting_started/install/deploy/deploy.md:10
#: dfd3b7c074124de78169f34168b7c757
msgid ""
"As our project has the ability to achieve ChatGPT performance of over "
"85%, there are certain hardware requirements. However, overall, the "
"project can be deployed and used on consumer-grade graphics cards. The "
"specific hardware requirements for deployment are as follows:"
msgstr "由于我们的项目有能力达到85%以上的ChatGPT性能，所以对硬件有一定的要求。但总体来说，我们在消费级的显卡上即可完成项目的部署使用，具体部署的硬件说明如下:"

#: ../../getting_started/install/deploy/deploy.md
#: 3d4530d981bf4dbab815a11c74bfd897
msgid "GPU"
msgstr "GPU"

#: ../../getting_started/install/deploy/deploy.md
#: 348c8f9b734244258416ea2e11b76caa f2f5e55b8c9b4c7da0ac090e763a9f47
msgid "VRAM Size"
msgstr "显存"

#: ../../getting_started/install/deploy/deploy.md
#: 9e099897409f42339bc284c378318a72
msgid "Performance"
msgstr "Performance"

#: ../../getting_started/install/deploy/deploy.md
#: 65bd67a198a84a5399f4799b505e062c
msgid "RTX 4090"
msgstr "RTX 4090"

#: ../../getting_started/install/deploy/deploy.md
#: 75e7f58f8f5d42f081a3e4d2e51ccc18 d897e949b37344d084f6917b977bcceb
msgid "24 GB"
msgstr "24 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 9cba72a2be3c41bda797ff447b63e448
msgid "Smooth conversation inference"
msgstr "Smooth conversation inference"

#: ../../getting_started/install/deploy/deploy.md
#: 90ea71d2099c47acac027773e69d2b23
msgid "RTX 3090"
msgstr "RTX 3090"

#: ../../getting_started/install/deploy/deploy.md
#: 7dd339251a1a45be9a45e0bb30bd09f7
msgid "Smooth conversation inference, better than V100"
msgstr "Smooth conversation inference, better than V100"

#: ../../getting_started/install/deploy/deploy.md
#: 8abe188052464e6aa395392db834c842
msgid "V100"
msgstr "V100"

#: ../../getting_started/install/deploy/deploy.md
#: 0b5263806a19446991cbd59c0fec6ba7 d645833d7e854eab81102374bf3fb7d8
msgid "16 GB"
msgstr "16 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 83bb5f40fa7f4e9389ac6abdb6bbb285 f9e58862a0cb488194d7ad536f359f0d
msgid "Conversation inference possible, noticeable stutter"
msgstr "Conversation inference possible, noticeable stutter"

#: ../../getting_started/install/deploy/deploy.md
#: 281f7b3c9a9f450798e5cc7612ea3890
msgid "T4"
msgstr "T4"

#: ../../getting_started/install/deploy/deploy.md:19
#: 7276d432615040b3a9eea3f2c5764319
msgid ""
"if your VRAM Size is not enough, DB-GPT supported 8-bit quantization and "
"4-bit quantization."
msgstr "如果你的显存不够，DB-GPT支持8-bit和4-bit量化版本"

#: ../../getting_started/install/deploy/deploy.md:21
#: f68d085e03d244ed9b5ccec347466889
msgid ""
"Here are some of the VRAM size usage of the models we tested in some "
"common scenarios."
msgstr "这里是量化版本的相关说明"

#: ../../getting_started/install/deploy/deploy.md
#: 8d397d7ee603448b97153192e6d3e372
msgid "Model"
msgstr "Model"

#: ../../getting_started/install/deploy/deploy.md
#: 4c789597627447278e89462690563faa
msgid "Quantize"
msgstr "Quantize"

#: ../../getting_started/install/deploy/deploy.md
#: a8d7c76224544ce69f376ac7cb2f5a3b dbf2beb53d7e491393b00848d63d7ffa
msgid "vicuna-7b-v1.5"
msgstr "vicuna-7b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 1bd5afee2af84597b5a423308e92362c 46d030b6ff1849ebb22b590e6978d914
#: 4cf8f3ebb0b743ffb2f1c123a25b75d0 6d96be424e6043daa2c02649894aa796
#: 83bb935f520c4c818bfe37e13034b2a7 92ba161085374917b7f82810b1a2bf00
#: ca5ccc49ba1046d2b4b13aaa7ceb62f5
msgid "4-bit"
msgstr "4-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 1173b5ee04cb4686ba34a527bc618bdb 558136997f4d49998f2f4e6a9bb656b0
#: 8d203a9a70684cbaa9d937af8450847f
msgid "8 GB"
msgstr "8 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 033458b772e3493b80041122e067e194 0f3eda083eac4739b2cf7d21337b145e
#: 6404eaa486cf45a69a27b0f87a7f6302 8e850aa3acf14ab2b231be74ddb34e86
#: ba258645f41f47a693aacbbc0f38e981 df67cac599234c4ba667a9b40eb6d9bc
#: fc07aeb321434722a320fed0afe3ffb8
msgid "8-bit"
msgstr "8-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 3ed2cb5787c14f268c446b03d5531233 68e7f0b0e8ad44ee86a86189bb3b553d
#: 8b4ea703d1df45c5be90d83c4723f16f cb606e0a458746fd86307c1e8aea08f1
#: d5da58dbde3c4bb4ac8b464a0a507c62 e8e140c610ec4971afe1b7ec2690382a
msgid "12 GB"
msgstr "12 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 512cd29c308c4d3ab66dbe63e7ea8f48 78f8307ab96c4245a1f09abcd714034c
msgid "vicuna-13b-v1.5"
msgstr "vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 48849903b3cb4888b6dd3d5efcbb24fb 83178c05f6cf431f82bb1d6d25b2645e
#: 979e3ab64df14753b0987bdd49bd5cc6
msgid "20 GB"
msgstr "20 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 0496046d1fb644c28361959b395231d7 3871c7c4432b4a15a9888586cdc70eda
msgid "llama-2-7b"
msgstr "llama-2-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 0c7c632d7c4d44fabfeed58fcc13db8f 78ee1adc6a7e4fa188706a1d5356059f
msgid "llama-2-13b"
msgstr "llama-2-13b"

#: ../../getting_started/install/deploy/deploy.md
#: 15a9341d4a6649908ef88045edd0cb93 d385381b0b4d4eff96a93a9d299cf516
msgid "llama-2-70b"
msgstr "llama-2-70b"

#: ../../getting_started/install/deploy/deploy.md
#: de37a5c2b02a498b9344b24f626db9dc
msgid "48 GB"
msgstr "48 GB"

#: ../../getting_started/install/deploy/deploy.md
#: e897098f81314ce4bf729aee1de7354c
msgid "80 GB"
msgstr "80 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 0782883260f840db8e8bf7c10b5ddf62 b03b5c9343454119ae11fcb2dedf9f90
msgid "baichuan-7b"
msgstr "baichuan-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 008a7d56e8dc4242ae3503bbbf4db153 65ea9ba20adb45519d65da7b16069fa8
msgid "baichuan-13b"
msgstr "baichuan-13b"

#: ../../getting_started/install/deploy/deploy.md:40
#: 1e434048c4844cc1906d83dd68af6d8c
msgid "2. Install"
msgstr "2. Install"

#: ../../getting_started/install/deploy/deploy.md:45
#: c6190ea13c024ddcb8e45ea22a235c3b
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration.  For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. [How to install "
"Miniconda](https://docs.conda.io/en/latest/miniconda.html)"
msgstr ""
"目前使用Sqlite作为默认数据库，因此DB-"
"GPT快速部署不需要部署相关数据库服务。如果你想使用其他数据库，需要先部署相关数据库服务。我们目前使用Miniconda进行python环境和包依赖管理[安装"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy/deploy.md:54
#: ee1e44044b73460ea8cd2f6c2eb6100d
msgid "Before use DB-GPT Knowledge"
msgstr "在使用知识库之前"

#: ../../getting_started/install/deploy/deploy.md:60
#: 6a9ff4138c69429bb159bf452fa7ee55
msgid ""
"Once the environment is installed, we have to create a new folder "
"\"models\" in the DB-GPT project, and then we can put all the models "
"downloaded from huggingface in this directory"
msgstr "如果你已经安装好了环境需要创建models, 然后到huggingface官网下载模型"

#: ../../getting_started/install/deploy/deploy.md:63
#: 1299b19bd0f24cc896c59e2c8e7e656c
msgid "Notice make sure you have install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:65
#: 69b3433c8e5c4cbb960e0178bdd6ac97
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:67
#: 50e3cfee5fd5484bb063d41693ac75f0
msgid "ubuntu:app-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:69
#: 81c85ca1188b4ef5b94e0431c6309f9b
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:86
#: 9b503ea553a24d488e1c180bf30055ff
msgid ""
"The model files are large and will take a long time to download. During "
"the download, let's configure the .env file, which needs to be copied and"
" created from the .env.template"
msgstr "模型文件很大，需要很长时间才能下载。在下载过程中，让我们配置.env文件，它需要从。env.template中复制和创建。"

#: ../../getting_started/install/deploy/deploy.md:88
#: 643b6a27bc0f43ee9451d18d52a9a2eb
msgid ""
"if you want to use openai llm service, see [LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"
msgstr ""
"如果想使用openai大模型服务, 可以参考[LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"

#: ../../getting_started/install/deploy/deploy.md:91
#: cc869640e66949e99faa17b1098b1306
msgid "cp .env.template .env"
msgstr "cp .env.template .env"

#: ../../getting_started/install/deploy/deploy.md:94
#: 1b94ed0e469f413b8e9d0ff3cdabca33
msgid ""
"You can configure basic parameters in the .env file, for example setting "
"LLM_MODEL to the model to be used"
msgstr "您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。"

#: ../../getting_started/install/deploy/deploy.md:96
#: 52cfa3636f2b4f949035d2d54b39a123
msgid ""
"([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。([Vicuna-v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5)， "
"目前Vicuna-v1.5模型(基于llama2)已经开源了，我们推荐你使用这个模型通过设置LLM_MODEL=vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md:98
#: 491fd44ede1645a3a2db10097c10dbe8
msgid "3. Run"
msgstr "3. Run"

#: ../../getting_started/install/deploy/deploy.md:100
#: f66b8a2b18b34df5b3e74674b4a9d7a9
msgid "1.Run db-gpt server"
msgstr "1.Run db-gpt server"

#: ../../getting_started/install/deploy/deploy.md:105
#: b72283f0ffdc4ecbb4da5239be5fd126
msgid "Open http://localhost:5000 with your browser to see the product."
msgstr "打开浏览器访问http://localhost:5000"

#: ../../getting_started/install/deploy/deploy.md:116
#: 1fae8a8ce4184feba2d74f877a25d8d2
msgid ""
"If you want to learn about dbgpt-webui, read https://github./csunny/DB-"
"GPT/tree/new-page-framework/datacenter"
msgstr ""
"如果你想了解web-ui, 请访问https://github./csunny/DB-GPT/tree/new-page-"
"framework/datacenter"

#: ../../getting_started/install/deploy/deploy.md:123
#: 573c0349bd2140e9bb356b53f1da6ee3
#, fuzzy
msgid "Multiple GPUs"
msgstr "4. Multiple GPUs"

#: ../../getting_started/install/deploy/deploy.md:125
#: af5d6a12ec954da19576decdf434df5d
msgid ""
"DB-GPT will use all available gpu by default. And you can modify the "
"setting `CUDA_VISIBLE_DEVICES=0,1` in `.env` file to use the specific gpu"
" IDs."
msgstr "DB-GPT默认加载可利用的gpu，你也可以通过修改 在`.env`文件 `CUDA_VISIBLE_DEVICES=0,1`来指定gpu IDs"

#: ../../getting_started/install/deploy/deploy.md:127
#: de96662007194418a2877cece51dc5cb
msgid ""
"Optionally, you can also specify the gpu ID to use before the starting "
"command, as shown below:"
msgstr "你也可以指定gpu ID启动"

#: ../../getting_started/install/deploy/deploy.md:137
#: 9cb0ff253fb2428dbaec97570e5c4fa4
msgid ""
"You can modify the setting `MAX_GPU_MEMORY=xxGib` in `.env` file to "
"configure the maximum memory used by each GPU."
msgstr "同时你可以通过在.env文件设置`MAX_GPU_MEMORY=xxGib`修改每个GPU的最大使用内存"

#: ../../getting_started/install/deploy/deploy.md:139
#: c708ee0a321444dd91be00cda469976c
#, fuzzy
msgid "Not Enough Memory"
msgstr "5. Not Enough Memory"

#: ../../getting_started/install/deploy/deploy.md:141
#: 760347ecf9a44d03a8e17cba153a2cc6
msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
msgstr "DB-GPT 支持 8-bit quantization 和 4-bit quantization."

#: ../../getting_started/install/deploy/deploy.md:143
#: 32e3dc941bfe4d6587e8be262f8fb4d3
msgid ""
"You can modify the setting `QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` "
"in `.env` file to use quantization(8-bit quantization is enabled by "
"default)."
msgstr "你可以通过在.env文件设置`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`"

#: ../../getting_started/install/deploy/deploy.md:145
#: bdc9a3788149427bac9f3cf35578e206
msgid ""
"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit"
" quantization can run with 48 GB of VRAM."
msgstr ""
"Llama-2-70b with 8-bit quantization 可以运行在80GB VRAM机器， 4-bit "
"quantization可以运行在 48 GB  VRAM"

#: ../../getting_started/install/deploy/deploy.md:147
#: 9b6085c41b5c4b96ac3e917dc5002fc2
msgid ""
"Note: you need to install the latest dependencies according to "
"[requirements.txt](https://github.com/eosphoros-ai/DB-"
"GPT/blob/main/requirements.txt)."
msgstr ""
"注意，需要安装[requirements.txt](https://github.com/eosphoros-ai/DB-"
"GPT/blob/main/requirements.txt)涉及的所有的依赖"

#~ msgid ""
#~ "Notice make sure you have install "
#~ "git-lfs centos:yum install git-lfs "
#~ "ubuntu:app-get install git-lfs "
#~ "macos:brew install git-lfs"
#~ msgstr ""
#~ "注意下载模型之前确保git-lfs已经安ubuntu:app-get install "
#~ "git-lfs macos:brew install git-lfs"

#~ msgid ""
#~ "You can refer to this document to"
#~ " obtain the Vicuna weights: "
#~ "[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md"
#~ "#model-weights) ."
#~ msgstr ""
#~ "你可以参考如何获取Vicuna weights文档[Vicuna](https://github.com/lm-"
#~ "sys/FastChat/blob/main/README.md#model-weights) ."

#~ msgid ""
#~ "If you have difficulty with this "
#~ "step, you can also directly use "
#~ "the model from [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) as "
#~ "a replacement."
#~ msgstr ""
#~ "如果觉得模型太大你也可以下载vicuna-7b [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) "

#~ msgid ""
#~ "If you want to access an external"
#~ " LLM service, you need to 1.set "
#~ "the variables LLM_MODEL=YOUR_MODEL_NAME "
#~ "MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in "
#~ "the .env file. 2.execute dbgpt_server.py "
#~ "in light mode"
#~ msgstr ""
#~ "如果你想访问外部的大模型服务(是通过DB-"
#~ "GPT/pilot/server/llmserver.py启动的模型服务)，1.需要在.env文件设置模型名和外部模型服务地址。2.使用light模式启动服务"


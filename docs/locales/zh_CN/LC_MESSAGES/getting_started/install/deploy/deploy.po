# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-17 14:35+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy/deploy.md:1
#: 73f932b662564edba45fbd711fd19005
msgid "Installation From Source"
msgstr "源码安装"

#: ../../getting_started/install/deploy/deploy.md:3
#: 70b623827a26447cb9382f1cb568b93c
msgid ""
"This tutorial gives you a quick walkthrough about use DB-GPT with you "
"environment and data."
msgstr "本教程为您提供了关于如何使用DB-GPT的使用指南。"

#: ../../getting_started/install/deploy/deploy.md:5
#: 6102ada4b19a4062947ad0ee5305dad5
msgid "Installation"
msgstr "安装"

#: ../../getting_started/install/deploy/deploy.md:7
#: 7c006c0c72944049bba43fd95daf1bd1
msgid "To get started, install DB-GPT with the following steps."
msgstr "请按照以下步骤安装DB-GPT"

#: ../../getting_started/install/deploy/deploy.md:9
#: eac8c7f921a042b79b4d0032c01b095a
msgid "1. Hardware Requirements"
msgstr "1. 硬件要求"

#: ../../getting_started/install/deploy/deploy.md:10
#: 8c430e2db5ce41e8b9d22e6e13c62cb3
msgid ""
"DB-GPT can be deployed on servers with low hardware requirements or on "
"servers with high hardware requirements."
msgstr "DB-GPT可以部署在对硬件要求不高的服务器，也可以部署在对硬件要求高的服务器"

#: ../../getting_started/install/deploy/deploy.md:12
#: a6b042509e1149fa8213a014e42eaaae
#, fuzzy
msgid "Low hardware requirements"
msgstr "1. 硬件要求"

#: ../../getting_started/install/deploy/deploy.md:13
#: 577c8c4edc2e4f45963b2a668385852f
msgid ""
"The low hardware requirements mode is suitable for integrating with "
"third-party LLM services' APIs, such as OpenAI, Tongyi, Wenxin, or "
"Llama.cpp."
msgstr "Low hardware requirements模式适用于对接第三方模型服务的api,比如OpenAI, 通义千问, 文心.cpp。"

#: ../../getting_started/install/deploy/deploy.md:15
#: 384475d3a87043eb9eebc384052ac9cc
msgid "DB-GPT provides set proxy api to support LLM api."
msgstr "DB-GPT可以通过设置proxy api来支持第三方大模型服务"

#: ../../getting_started/install/deploy/deploy.md:17
#: e5bd8a999adb4e07b8b5221f1893251d
msgid "As our project has the ability to achieve ChatGPT performance of over 85%,"
msgstr "由于我们的项目有能力达到85%以上的ChatGPT性能"

#: ../../getting_started/install/deploy/deploy.md:19
#: 6a97ed5893414e17bb9c1f8bb21bc965
#, fuzzy
msgid "High hardware requirements"
msgstr "1. 硬件要求"

#: ../../getting_started/install/deploy/deploy.md:20
#: d0c248939b4143a2b01afd051b02ec12
#, fuzzy
msgid ""
"The high hardware requirements mode is suitable for independently "
"deploying LLM services, such as Llama series models, Baichuan, ChatGLM, "
"Vicuna, and other private LLM service. there are certain hardware "
"requirements. However, overall, the project can be deployed and used on "
"consumer-grade graphics cards. The specific hardware requirements for "
"deployment are as follows:"
msgstr "High hardware requirements模式适用于需要独立部署私有大模型服务，比如Llama系列模型，Baichuan, chatglm，vicuna等私有大模型所以对硬件有一定的要求。但总体来说，我们在消费级的显卡上即可完成项目的部署使用，具体部署的硬件说明如下:"

#: ../../getting_started/install/deploy/deploy.md
#: 2ee432394f6b4d9cb0a424f4b99bf3be
msgid "GPU"
msgstr "GPU"

#: ../../getting_started/install/deploy/deploy.md
#: 4cd716486f994080880f84853b047a5d
msgid "VRAM Size"
msgstr "显存"

#: ../../getting_started/install/deploy/deploy.md
#: d1b33d0348894bfc8a843a3d38c6daaa
msgid "Performance"
msgstr "Performance"

#: ../../getting_started/install/deploy/deploy.md
#: d5850bbe7d0a430d993b7e6bd1f24bff
msgid "RTX 4090"
msgstr "RTX 4090"

#: ../../getting_started/install/deploy/deploy.md
#: c7d15be08ac74624bbfb5eb4554fc7ff
msgid "24 GB"
msgstr "24 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 219dff2fee83460da55d9d628569365e
msgid "Smooth conversation inference"
msgstr "丝滑的对话体验"

#: ../../getting_started/install/deploy/deploy.md
#: 56025c5f37984963943de7accea85850
msgid "RTX 3090"
msgstr "RTX 3090"

#: ../../getting_started/install/deploy/deploy.md
#: 8a0b8a0afa0c4cc39eb7c2271775cf60
msgid "Smooth conversation inference, better than V100"
msgstr "丝滑的对话体验，性能好于V100"

#: ../../getting_started/install/deploy/deploy.md
#: 2fc5e6ac8a6b4c508944c659adffa0c1
msgid "V100"
msgstr "V100"

#: ../../getting_started/install/deploy/deploy.md
#: f92a1393539a49db983b06f7276f446b
msgid "16 GB"
msgstr "16 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 4e7de52a58d24a0bb10e45e1435128a6
msgid "Conversation inference possible, noticeable stutter"
msgstr "Conversation inference possible, noticeable stutter"

#: ../../getting_started/install/deploy/deploy.md
#: 217fe55f590a497ba6622698945e7be8
msgid "T4"
msgstr "T4"

#: ../../getting_started/install/deploy/deploy.md:30
#: 30ca67fe27f64df093a2d281e1288c5c
#, fuzzy
msgid ""
"If your VRAM Size is not enough, DB-GPT supported 8-bit quantization and "
"4-bit quantization."
msgstr "如果你的显存不够，DB-GPT支持8-bit和4-bit量化版本"

#: ../../getting_started/install/deploy/deploy.md:32
#: fc0c3a0730d64e9e98d1b25f4dd5db34
msgid ""
"Here are some of the VRAM size usage of the models we tested in some "
"common scenarios."
msgstr "这里是量化版本的相关说明"

#: ../../getting_started/install/deploy/deploy.md
#: 1f1f6c10209b446f99d520fdb68e0f5d
msgid "Model"
msgstr "Model"

#: ../../getting_started/install/deploy/deploy.md
#: 18e3240d407e41f88028b24aeced1bf4
msgid "Quantize"
msgstr "Quantize"

#: ../../getting_started/install/deploy/deploy.md
#: 03aa79d3c3f54e3c834180b0d1ed9a5c
msgid "vicuna-7b-v1.5"
msgstr "vicuna-7b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 09419ad0a88c4179979505ef71204fd6 1b4ab0186184493d895eeec12d078c52
#: 6acec7b76e604343885aa71d92b04d1e 9b73ca1c18d14972b894db69438e3fb2
#: b869995505ae4895b9f13e271470e5cb c9eaf983eeb2486da08e628728ae301f
#: ff0a86dc63ce4cd580f354d15d333501
msgid "4-bit"
msgstr "4-bit"

#: ../../getting_started/install/deploy/deploy.md
#: d0d959f022f44bbeb34d67ccf49ba3bd
msgid "8 GB"
msgstr "8 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 01cb7be0064940e8a637df7ed8e15310 13568d8a793b4c2db655f89dc690929a
#: 28ce31711f91455b9b910276fa059c65 2dddf2e87a70452fb27a627d62464346
#: 3f3f4dc00acb43258dce311f144e0fd7 5aa76fd2fb35474e8d06795e7369ceb4
#: d660be499efc4b6ca61da0d5af758620
msgid "8-bit"
msgstr "8-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 3b963a1ce6934229ba7658cb407b6a52
msgid "12 GB"
msgstr "12 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 30d28dcaa64545198aaa20fe4562bb6d
msgid "vicuna-13b-v1.5"
msgstr "vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 28de25a1952049d2b7aff41020e428ff
msgid "20 GB"
msgstr "20 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 535974c886b14c618ca84de1fe63d5e4
msgid "llama-2-7b"
msgstr "llama-2-7b"

#: ../../getting_started/install/deploy/deploy.md
#: cc04760a8b9e4a79a7dada9a11abda2c
msgid "llama-2-13b"
msgstr "llama-2-13b"

#: ../../getting_started/install/deploy/deploy.md
#: 9e83d8d5ae44411dba4cc6c2d796b20f
msgid "llama-2-70b"
msgstr "llama-2-70b"

#: ../../getting_started/install/deploy/deploy.md
#: cb6ce389adfc463a9c851eb1e4abfcff
msgid "48 GB"
msgstr "48 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 906d664156084223a4efa0ae9804bd33
msgid "80 GB"
msgstr "80 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 957fb0c6f3114a63ba33a1cfb31060e3
msgid "baichuan-7b"
msgstr "baichuan-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 5f3bc4cf57d946cfb38a941250685151
msgid "baichuan-13b"
msgstr "baichuan-13b"

#: ../../getting_started/install/deploy/deploy.md:51
#: 87ae8c58df314b69ae119aa831cb7dd5
msgid "2. Install"
msgstr "2. Install"

#: ../../getting_started/install/deploy/deploy.md:56
#: 79cdebf089614761bf4299a9ce601b81
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration.  For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. [How to install "
"Miniconda](https://docs.conda.io/en/latest/miniconda.html)"
msgstr ""
"目前使用Sqlite作为默认数据库，因此DB-"
"GPT快速部署不需要部署相关数据库服务。如果你想使用其他数据库，需要先部署相关数据库服务。我们目前使用Miniconda进行python环境和包依赖管理[安装"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy/deploy.md:65
#: 03ff2f444721454588095bb348220276
msgid "Before use DB-GPT Knowledge"
msgstr "在使用知识库之前"

#: ../../getting_started/install/deploy/deploy.md:71
#: b6faa4d078a046d6a7c0313e8deef0f3
msgid ""
"Once the environment is installed, we have to create a new folder "
"\"models\" in the DB-GPT project, and then we can put all the models "
"downloaded from huggingface in this directory"
msgstr "如果你已经安装好了环境需要创建models, 然后到huggingface官网下载模型"

#: ../../getting_started/install/deploy/deploy.md:74
#: f43fd2b74d994bf6bb4016e88c43d51a
msgid "Notice make sure you have install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:76
#: f558a7ee728a4344af576aa375b43092
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:78
#: bab08604a3ba45b9b827ff5a4b931601
msgid "ubuntu:apt-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:80
#: b4a107e5f8524acc9aed74318880f9f3
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:97
#: ecb5fa1f18154685bb4336d04ac3a386
msgid ""
"The model files are large and will take a long time to download. During "
"the download, let's configure the .env file, which needs to be copied and"
" created from the .env.template"
msgstr "模型文件很大，需要很长时间才能下载。在下载过程中，让我们配置.env文件，它需要从。env.template中复制和创建。"

#: ../../getting_started/install/deploy/deploy.md:99
#: 0f08b0ecbea14cbdba29ea8d87cf24b4
msgid ""
"if you want to use openai llm service, see [LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"
msgstr ""
"如果想使用openai大模型服务, 可以参考[LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"

#: ../../getting_started/install/deploy/deploy.md:102
#: 6efb9a45ab2c45c7b4770f987b639c52
msgid "cp .env.template .env"
msgstr "cp .env.template .env"

#: ../../getting_started/install/deploy/deploy.md:105
#: b9d2b81a2cf440c3b49a5c06759eb2ba
msgid ""
"You can configure basic parameters in the .env file, for example setting "
"LLM_MODEL to the model to be used"
msgstr "您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。"

#: ../../getting_started/install/deploy/deploy.md:107
#: 2f6afa40ca994115b16ba28baaf65bde
msgid ""
"([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。([Vicuna-v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5)， "
"目前Vicuna-v1.5模型(基于llama2)已经开源了，我们推荐你使用这个模型通过设置LLM_MODEL=vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md:109
#: 7c5883f9594646198f464e6dafb2f0ff
msgid "3. Run"
msgstr "3. Run"

#: ../../getting_started/install/deploy/deploy.md:111
#: 0e3719a238eb4332b7c15efa3f16e3e2
msgid "**(Optional) load examples into SQLlite**"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:116
#: c901055131ce4688b1c602393913b675
msgid "On windows platform:"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:121
#: 777a50f9167c4b8f9c2a96682ccc4c4a
msgid "1.Run db-gpt server"
msgstr "1.Run db-gpt server"

#: ../../getting_started/install/deploy/deploy.md:127
#: 62aafb652df8478281ab633d8d082e7f
msgid "Open http://localhost:5000 with your browser to see the product."
msgstr "打开浏览器访问http://localhost:5000"

#: ../../getting_started/install/deploy/deploy.md:130
#: cff18fc20ffd4716bc7cf377730dd5ec
msgid "If you want to access an external LLM service, you need to"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:132
#: f27c3aa9e627480a96cd04fcd4bfdaec
msgid ""
"1.set the variables LLM_MODEL=YOUR_MODEL_NAME, "
"MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in the .env "
"file."
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:134
#: e05a395f67924514929cd025fab67e44
msgid "2.execute dbgpt_server.py in light mode"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:137
#: a5d7fcb46ba446bf9913646b28b036ed
msgid ""
"If you want to learn about dbgpt-webui, read https://github./csunny/DB-"
"GPT/tree/new-page-framework/datacenter"
msgstr ""
"如果你想了解web-ui, 请访问https://github./csunny/DB-GPT/tree/new-page-"
"framework/datacenter"

#: ../../getting_started/install/deploy/deploy.md:143
#: 90c614e7744c4a7f843adb8968b58c78
#, fuzzy
msgid "Multiple GPUs"
msgstr "4. Multiple GPUs"

#: ../../getting_started/install/deploy/deploy.md:145
#: 7b72e7cbd9d246299de5986772df4825
msgid ""
"DB-GPT will use all available gpu by default. And you can modify the "
"setting `CUDA_VISIBLE_DEVICES=0,1` in `.env` file to use the specific gpu"
" IDs."
msgstr "DB-GPT默认加载可利用的gpu，你也可以通过修改 在`.env`文件 `CUDA_VISIBLE_DEVICES=0,1`来指定gpu IDs"

#: ../../getting_started/install/deploy/deploy.md:147
#: b7e2f7bbf625464489b3fd9aedb0ed59
msgid ""
"Optionally, you can also specify the gpu ID to use before the starting "
"command, as shown below:"
msgstr "你也可以指定gpu ID启动"

#: ../../getting_started/install/deploy/deploy.md:157
#: 69fd2183a143428fb77949f58381d455
msgid ""
"You can modify the setting `MAX_GPU_MEMORY=xxGib` in `.env` file to "
"configure the maximum memory used by each GPU."
msgstr "同时你可以通过在.env文件设置`MAX_GPU_MEMORY=xxGib`修改每个GPU的最大使用内存"

#: ../../getting_started/install/deploy/deploy.md:159
#: 6cd03b9728f943a4a632aa9b061931f0
#, fuzzy
msgid "Not Enough Memory"
msgstr "5. Not Enough Memory"

#: ../../getting_started/install/deploy/deploy.md:161
#: 4837aba4c80b42819c1a6345de0aa820
msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
msgstr "DB-GPT 支持 8-bit quantization 和 4-bit quantization."

#: ../../getting_started/install/deploy/deploy.md:163
#: c1a701e9bc4c4439adfb930d0e953cec
msgid ""
"You can modify the setting `QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` "
"in `.env` file to use quantization(8-bit quantization is enabled by "
"default)."
msgstr "你可以通过在.env文件设置`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`"

#: ../../getting_started/install/deploy/deploy.md:165
#: 205c101f1f774130a5853dd9b7373d36
msgid ""
"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit"
" quantization can run with 48 GB of VRAM."
msgstr ""
"Llama-2-70b with 8-bit quantization 可以运行在80GB VRAM机器， 4-bit "
"quantization可以运行在 48 GB  VRAM"

#~ msgid ""
#~ "Notice make sure you have install "
#~ "git-lfs centos:yum install git-lfs "
#~ "ubuntu:app-get install git-lfs "
#~ "macos:brew install git-lfs"
#~ msgstr ""
#~ "注意下载模型之前确保git-lfs已经安ubuntu:app-get install "
#~ "git-lfs macos:brew install git-lfs"

#~ msgid ""
#~ "You can refer to this document to"
#~ " obtain the Vicuna weights: "
#~ "[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md"
#~ "#model-weights) ."
#~ msgstr ""
#~ "你可以参考如何获取Vicuna weights文档[Vicuna](https://github.com/lm-"
#~ "sys/FastChat/blob/main/README.md#model-weights) ."

#~ msgid ""
#~ "If you have difficulty with this "
#~ "step, you can also directly use "
#~ "the model from [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) as "
#~ "a replacement."
#~ msgstr ""
#~ "如果觉得模型太大你也可以下载vicuna-7b [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) "

#~ msgid ""
#~ "If you want to access an external"
#~ " LLM service, you need to 1.set "
#~ "the variables LLM_MODEL=YOUR_MODEL_NAME "
#~ "MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in "
#~ "the .env file. 2.execute dbgpt_server.py "
#~ "in light mode"
#~ msgstr ""
#~ "如果你想访问外部的大模型服务(是通过DB-"
#~ "GPT/pilot/server/llmserver.py启动的模型服务)，1.需要在.env文件设置模型名和外部模型服务地址。2.使用light模式启动服务"

#~ msgid ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."
#~ msgstr ""
#~ "注意，需要安装[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)涉及的所有的依赖"

#~ msgid "ubuntu:app-get install git-lfs"
#~ msgstr ""

